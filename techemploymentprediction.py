# -*- coding: utf-8 -*-
"""Malabuyo_ECON464TechEmploymentPrediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15flghYIftU7Xemtn2tCPkiGvWtUsSeiL
"""

pip install seaborn

pip install xgboost

"""![PROGRAMMING JOB EMPLOYMENT](https://unsplash.com/photos/group-of-people-using-laptop-computer-QckxruozjRg)

#  **"Comparative Analysis of Classification Models for Predicting Employed Status of Programmers"**

# **Importing all the libraries and Loading the dataset**
"""

import pandas as pd
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt
from sklearn import preprocessing
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, auc


from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score


from sklearn.ensemble import RandomForestClassifier

from xgboost import XGBClassifier

df = pd.read_csv(("techjobs.csv"))

df.head()

"""# **Exploratory Data Analysis**"""

df.info()

# Remove Unnamed: 0 column
df.drop(columns = 'Unnamed: 0', inplace=True)

df.info()

df.shape

df.describe()

df.dtypes

# Check for missing values and count them for each column
missing_values = df.isna().sum()

# Display the count of missing values for each column
print(missing_values)

languages = ["JavaScript", "C/C++", "C#" "Java", "Python", "SQL"]

def checkLanguages(data):
  for i in data.rows:
    string_to_check = i["HaveWorkedWith"]
    cleaned_string = string_to_check.Split(";")
    # check if one of the languages is in the cleaned string and add 1 if yes and 0 if no

#drop the column as it has many null values, plus the computer skills column has inferred it; plus it has multiple  values
df.drop(columns = 'HaveWorkedWith', inplace=True)

df.head()

# Check for missing values and count them for each column
missing_values = df.isna().sum()

# Display the count of missing values for each column
print(missing_values)

df.dtypes

#Check the number of unique value from all of the object datatype
df.select_dtypes(include='object').nunique()

#company has many unique values; lets visualize them
df.Country.unique()

# Define a function to segment countries into continents
def segment_country(country):
    if country in ['United States of America', 'Canada', 'Mexico']:
        return 'NorthAmerica'
    elif country in ['United Kingdom of Great Britain and Northern Ireland', 'France', 'Germany', 'Spain', 'Italy', 'Portugal', 'Belgium', 'Netherlands', 'Austria', 'Switzerland', 'Denmark', 'Ireland', 'Norway', 'Sweden', 'Finland', 'Greece', 'Czech Republic', 'Slovakia', 'Hungary', 'Poland']:
        return 'Europe'
    elif country in ['Brazil', 'Argentina', 'Chile', 'Colombia', 'Peru', 'Venezuela, Bolivarian Republic of...', 'Bolivia']:
        return 'South America'
    elif country in ['China', 'Japan', 'South Korea', 'Viet Nam', 'India', 'Sri Lanka', 'Pakistan', 'Bangladesh', 'Indonesia', 'Malaysia', 'Philippines', 'Taiwan', 'Thailand', 'Cambodia', 'Myanmar', 'Laos', 'Singapore', 'Hong Kong (S.A.R.)']:
        return 'Asia'
    elif country in ['Australia', 'New Zealand', 'Fiji', 'Papua New Guinea', 'Solomon Islands', 'Vanuatu', 'Samoa', 'Tonga']:
        return 'Australia'
    else:
        return 'Others'

# create a new column 'Continent'
df['Continent'] = df['Country'].apply(segment_country)

df.Continent.unique()

continent_counts = df['Continent'].value_counts()

# Create a pie chart
plt.figure(figsize=(8, 8))
plt.pie(continent_counts, labels=continent_counts.index, autopct='%1.1f%%', startangle=140, colors=['skyblue', 'lightcoral', 'lightgreen', 'lightgray', 'lightpink', 'lightyellow'])
plt.title('Distribution of Countries by Continent')
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Display the plot
plt.show()

# Drop Country
df.drop(columns = 'Country', inplace=True)
# Similarly remove YearsCodePro column as we have YearsCode column
df.drop(columns = 'YearsCodePro', inplace=True)

df.head()

"""Visualizing the attributes

**Age Distribution**
"""

plt.figure(figsize=(10, 6))
sns.countplot(data=df, x='Age', order=['<35', '>35'])
plt.xlabel('Age Group')
plt.ylabel('Count')
plt.title('Age Distribution of Respondents')
plt.xticks(rotation=45)
plt.show()

"""**Distribution of Education Levels**"""

plt.figure(figsize=(8, 6))
sns.countplot(data=df, x='EdLevel', order=df['EdLevel'].value_counts().index)
plt.xticks(rotation=90)
plt.xlabel('Education Level')
plt.ylabel('Count')
plt.title('Distribution of Education Levels')
plt.show()

"""**Distribution of Employment Status**"""

plt.figure(figsize=(8, 6))
sns.countplot(data=df, x='Employment', order=df['Employment'].value_counts().index)
plt.xticks(rotation=90)
plt.xlabel('Employment Status')
plt.ylabel('Count')
plt.title('Distribution of Employment Status')
plt.show()

"""**'Relationship Between Age and Employment Status'**"""

plt.figure(figsize=(8, 6))
sns.countplot(data=df, x='Age', hue='Employment', palette='Set1')
plt.xlabel('Age')
plt.ylabel('Count')
plt.title('Relationship Between Age and Employment Status')
plt.show()

"""**Distribution of Countries by Continent**"""

continent_counts = df['Continent'].value_counts()

# Create a bar plot
plt.figure(figsize=(10, 6))
plt.bar(continent_counts.index, continent_counts.values, color='skyblue')
plt.xlabel('Continent')
plt.ylabel('Count')
plt.title('Distribution of Countries by Continent')
plt.xticks(rotation=45)  # Rotate x-axis labels for better readability
plt.tight_layout()

plt.show()

"""**Distribution of Target variable "Employed"**"""

plt.figure(figsize=(8, 6))
sns.countplot(data=df, x='Employed', order=df['Employed'].value_counts().index)
plt.xticks(rotation=90)
plt.xlabel('Employed')
plt.ylabel('Count')
plt.title('Distribution of Employed')
plt.show()

"""**Average Previous Salary by Continent**"""

# Group the data by 'Continent' and calculate the average 'ComputerSkills' for each continent
continent_computer_skills = df.groupby('Continent')['ComputerSkills'].mean().reset_index()

# Sort the data by average 'ComputerSkills' in descending order to show the highest skilled continents first
continent_computer_skills = continent_computer_skills.sort_values(by='ComputerSkills', ascending=False)

# Create a bar plot to visualize the average 'ComputerSkills' by continent
# Create a bar plot to visualize the average 'ComputerSkills' by continent
plt.figure(figsize=(10, 6))
sns.barplot(data=continent_computer_skills, x='ComputerSkills', y='Continent')
plt.xlabel('Average Computer Skills')
plt.ylabel('Continent')
plt.title('Average Computer Skills by Continent')
plt.show()

"""**Continents with Highest Employment Percentage**"""

# Calculate the employment percentage for each continent
continent_employment = df.groupby('Continent')['Employed'].mean().reset_index()

# Sort the data by employment percentage in descending order to show the highest percentages first
continent_employment = continent_employment.sort_values(by='Employed', ascending=False)

# Create a bar plot to visualize the employment percentage by continent
plt.figure(figsize=(10, 6))
sns.barplot(data=continent_employment, x='Employed', y='Continent')
plt.xlabel('Employment Percentage')
plt.ylabel('Continent')
plt.title('Continents with Highest Employment Percentage')
plt.show()

"""# **Check the Co-relation of the Attributes**"""

df.dtypes

df.head()

# Create a copy of the original dataframe
df_copy = df.copy()

# Label encode categorical columns
label_encoder = LabelEncoder()
categorical_columns = ['Age', 'Accessibility', 'EdLevel', 'Gender', 'MentalHealth', 'MainBranch', 'Continent']
for col in categorical_columns:
    df_copy[col] = label_encoder.fit_transform(df[col])

df_copy.head()

# Calculate the correlation matrix
correlation_matrix = df_copy.corr()

print(correlation_matrix)

# Create a heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, linewidths=0.5)
plt.title('Correlation Heatmap')
plt.show()

"""# **Check for class imbalance and sampling, Remove outliers**"""

#Check the Class Variable
df_copy['Employed'].value_counts()

# Plot the distributions of 'YearsCode', 'PreviousSalary,' and 'ComputerSkills'
plt.figure(figsize=(15, 5))
plt.subplot(131)
sns.boxplot(data=df_copy, y='YearsCode', orient='vertical')
plt.title('YearsCode Distribution')
plt.subplot(132)
sns.boxplot(data=df_copy, y='PreviousSalary', orient='vertical')
plt.title('PreviousSalary Distribution')
plt.subplot(133)
sns.boxplot(data=df_copy, y='ComputerSkills', orient='vertical')
plt.title('ComputerSkills Distribution')
plt.show()

# Define a function to remove outliers using IQR
def remove_outliers_iqr(data, column_name):
    Q1 = data[column_name].quantile(0.25)
    Q3 = data[column_name].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    data = data[(data[column_name] >= lower_bound) & (data[column_name] <= upper_bound)]
    return data

# Remove outliers in 'YearsCode', 'PreviousSalary,' and 'ComputerSkills'
df_copy = remove_outliers_iqr(df_copy, 'YearsCode')
df_copy = remove_outliers_iqr(df_copy, 'PreviousSalary')
df_copy = remove_outliers_iqr(df_copy, 'ComputerSkills')

df_copy.head()

"""# **Create a train-test split**"""

# Step 1: Define features and target variable
X = df_copy.drop("Employed", axis=1)  # Features (all columns except 'Employed')
y = df_copy["Employed"]  # Target variable

# Step 2: Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Training set size:", len(X_train))
print("Testing set size:", len(X_test))

print(X_train)

"""# **MODEL FITTING**

# **Logistic Regression**
"""

# Create a Logistic Regression classifier
logistic_regression = LogisticRegression(random_state=0)
logistic_regression.fit(X_train, y_train)
y_pred_lr = logistic_regression.predict(X_test)

# Calculate the AUC
y_scores_lr = logistic_regression.predict_proba(X_train)[:, 1]
fpr_lr, tpr_lr, thresholds_lr = roc_curve(y_train, y_scores_lr)
auc_lr = auc(fpr_lr, tpr_lr)

# Print the AUC
print(f'Logistic Regression - AUC: {auc_lr:.4f}')

# Plot the confusion matrix
cm_lr = confusion_matrix(y_test, y_pred_lr)
plt.figure()
sns.heatmap(cm_lr, annot=True, fmt="d", cmap="Blues", xticklabels=['Not Employed', 'Employed'], yticklabels=['Not Employed', 'Employed'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Logistic Regression - Confusion Matrix')
plt.show()

# Plot the ROC curve
plt.figure()
plt.plot(fpr_lr, tpr_lr, color='darkorange', lw=2, label=f'ROC curve (area = {auc_lr:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Logistic Regression - ROC Curve')
plt.legend(loc='lower right')
plt.show()

"""# **Decision Tree Classifier**"""

# Define the parameter grid for grid search
param_grid = {
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Create a Decision Tree classifier
decision_tree = DecisionTreeClassifier(random_state=42)

# Create a GridSearchCV object
grid_search = GridSearchCV(estimator=decision_tree, param_grid=param_grid, cv=5, scoring='roc_auc', n_jobs=-1)

# Perform the grid search
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_

# Use the best hyperparameters to create and train the Decision Tree model
best_decision_tree = DecisionTreeClassifier(random_state=42, **best_params)
best_decision_tree.fit(X_train, y_train)

# Predict using the best Decision Tree model
y_pred_dt = best_decision_tree.predict(X_test)

# Calculate the AUC
y_scores_dt = best_decision_tree.predict_proba(X_train)[:, 1]
fpr_dt, tpr_dt, thresholds_dt = roc_curve(y_train, y_scores_dt)
auc_dt = auc(fpr_dt, tpr_dt)

print(auc_dt)

# Print the best hyperparameters and AUC
print(f'Best Hyperparameters for Decision Tree: {best_params}')
print(f'Decision Tree - AUC with Best Hyperparameters: {auc_dt:.4f}')

# Plot the ROC curve
plt.figure()
plt.plot(fpr_dt, tpr_dt, color='darkorange', lw=2, label=f'ROC curve (area = {auc_dt:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Decision Tree with Best Hyperparameters - ROC Curve')
plt.legend(loc='lower right')
plt.show()

# Plot the confusion matrix
cm_dt = confusion_matrix(y_test, y_pred_dt)
plt.figure()
sns.heatmap(cm_dt, annot=True, fmt="d", cmap="Blues", xticklabels=['Not Employed', 'Employed'], yticklabels=['Not Employed', 'Employed'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Decision Tree with Best Hyperparameters - Confusion Matrix')
plt.show()

print('F-1 Score : ',(f1_score(y_test, y_pred_dt, average='micro')))
print('Precision Score : ',(precision_score(y_test, y_pred_dt, average='micro')))
print('Recall Score : ',(recall_score(y_test, y_pred_dt, average='micro')))

"""# **Random Forest Classifier**"""

# Define the parameter grid for grid search
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Create a Random Forest classifier
random_forest = RandomForestClassifier(random_state=42)

# Create a GridSearchCV object
grid_search = GridSearchCV(estimator=random_forest, param_grid=param_grid, cv=5, scoring='roc_auc', n_jobs=-1)

# Perform the grid search
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_

# Use the best hyperparameters to create and train the Random Forest model
best_random_forest = RandomForestClassifier(random_state=42, **best_params)
best_random_forest.fit(X_train, y_train)

# Predict using the best Random Forest model
y_pred_rf = best_random_forest.predict(X_test)

# Calculate the AUC
y_scores_rf = best_random_forest.predict_proba(X_train)[:, 1]
fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_train, y_scores_rf)
auc_rf = auc(fpr_rf, tpr_rf)

# Print the best hyperparameters and AUC
print(f'Best Hyperparameters: {best_params}')
print(f'Random Forest - AUC with Best Hyperparameters: {auc_rf:.4f}')

# Plot the ROC curve
plt.figure()
plt.plot(fpr_rf, tpr_rf, color='darkorange', lw=2, label=f'ROC curve (area = {auc_rf:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Random Forest with Best Hyperparameters - ROC Curve')
plt.legend(loc='lower right')
plt.show()

# Plot the confusion matrix
cm_rf = confusion_matrix(y_test, y_pred_rf)
plt.figure()
sns.heatmap(cm_rf, annot=True, fmt="d", cmap="Blues", xticklabels=['Not Employed', 'Employed'], yticklabels=['Not Employed', 'Employed'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Random Forest with Best Hyperparameters - Confusion Matrix')
plt.show()

print('F-1 Score : ',(f1_score(y_test, y_pred_rf, average='micro')))
print('Precision Score : ',(precision_score(y_test, y_pred_rf, average='micro')))
print('Recall Score : ',(recall_score(y_test, y_pred_rf, average='micro')))

"""# **XGBoost Classifier**"""

# Define the parameter grid for grid search
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.2]
}

# Create an XGBoost classifier
xgboost = XGBClassifier(random_state=42)

# Create a GridSearchCV object
grid_search = GridSearchCV(estimator=xgboost, param_grid=param_grid, cv=5, scoring='roc_auc', n_jobs=-1)

# Perform the grid search
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params_xgb = grid_search.best_params_

# Use the best hyperparameters to create and train the XGBoost model
best_xgboost = XGBClassifier(random_state=42, **best_params_xgb)
best_xgboost.fit(X_train, y_train)

# Predict using the best XGBoost model
y_pred_xgb = best_xgboost.predict(X_test)

# Calculate the AUC
y_scores_xgb = best_xgboost.predict_proba(X_train)[:, 1]
fpr_xgb, tpr_xgb, thresholds_xgb = roc_curve(y_train, y_scores_xgb)
auc_xgb = auc(fpr_xgb, tpr_xgb)

# Print the best hyperparameters and AUC
print(f'Best Hyperparameters for XGBoost: {best_params_xgb}')
print(f'XGBoost - AUC with Best Hyperparameters: {auc_xgb:.4f}')

# Plot the ROC curve
plt.figure()
plt.plot(fpr_xgb, tpr_xgb, color='darkorange', lw=2, label=f'ROC curve (area = {auc_xgb:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('XGBoost with Best Hyperparameters - ROC Curve')
plt.legend(loc='lower right')
plt.show()

# Plot the confusion matrix
cm_xgb = confusion_matrix(y_test, y_pred_xgb)
plt.figure()
sns.heatmap(cm_xgb, annot=True, fmt="d", cmap="Blues", xticklabels=['Not Employed', 'Employed'], yticklabels=['Not Employed', 'Employed'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('XGBoost with Best Hyperparameters - Confusion Matrix')
plt.show()

print('F-1 Score : ',(f1_score(y_test, y_pred_xgb, average='micro')))
print('Precision Score : ',(precision_score(y_test, y_pred_xgb, average='micro')))
print('Recall Score : ',(recall_score(y_test, y_pred_xgb, average='micro')))

"""# **PLOT THE AUC FOR ALL MODELS**"""

# AUC values for each model
auc_values = [auc_lr, auc_dt, auc_rf, auc_xgb]

# Model names
model_names = ["Logistic Regression", "Decision Tree", "Random Forest", "XGBoost"]

# Plot the AUC values
plt.figure(figsize=(10, 6))
plt.plot(model_names, auc_values, marker='o', linestyle='-')
plt.title('Comparative AUC Curve for Models')
plt.xlabel('Models')
plt.ylabel('AUC')
plt.grid(True)
plt.show()